{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores.cassandra import Cassandra\n",
    "import cassio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.astradb import AstraDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "api=os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "astradb_app_token=\"AstraCS:CodRLeAMALWQPtZuZJOMrodz:510e65811c5303f52fe4c16568b7b4bc8d2c175244190373ecd80b0f5ed35b6f\"\n",
    "astradb_id=\"76951b4e-29b4-450f-bb6a-7a669109eebb\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cassio.init(token=astradb_app_token,database_id=astradb_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=WebBaseLoader(web_path=\"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "                     bs_kwargs=dict(parse_only=bs4.SoupStrainer(class_=('post-title','post-content','post-header'))))\n",
    "\n",
    "docs=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "final_docs=splitter.split_documents(docs)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=OllamaEmbeddings()\n",
    "\n",
    "astra_vectordtore=Cassandra(\n",
    "    embedding=embeddings,\n",
    "    table_name='qa_mini_demo',\n",
    "    session=None,\n",
    "    keyspace=None\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(final_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "astra_vectordtore.add_documents(final_docs)\n",
    "astradb_index=VectorStoreIndexWrapper(vectorstore=astra_vectordtore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "prompt=ChatPromptTemplate.from_template('''\n",
    "Answer the following question using only the provided context.\n",
    "think step-by-step before generating a response.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "\n",
    "Question:{input}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Tree of Thoughts (Yao et al. 2023) is a method that extends Chain of Thought (CoT) by exploring multiple reasoning possibilities at each step, creating a tree structure of thought steps. This can be used for complex tasks, with task decomposition that can be done by a large language model (LLM), task-specific instructions, or human inputs.\\n\\nThe Tree of Thoughts can be used in an autonomous agent system, as shown in Fig. 1, where the first component is Planning. This includes task decomposition, which is also a component of the Chain of Thought (CoT) technique.\\n\\nAnother related method is Chain of Hindsight (CoH; Liu et al. 2023), which encourages the model to improve its own outputs by presenting it with a sequence of past outputs, each annotated with feedback.\\n\\nThe model's memory includes short-term memory, which utilizes in-context learning, and long-term memory, which allows the agent to retain and recall information over extended periods.\\n\\nThe agent can also learn to call external APIs for extra information, which is useful for tasks that require information that is not available in the model's weights.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "astradb_index.query(\"Task Decomposition\",llm=ChatGroq(api_key=api))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "retriever=astra_vectordtore.as_retriever()\n",
    "doc_chain=create_stuff_documents_chain(ChatGroq(api_key=api),prompt)\n",
    "ret_chain=create_retrieval_chain(retriever,doc_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['Cassandra', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.cassandra.Cassandra object at 0x0000022EFFCED750>), config={'run_name': 'retrieve_documents'})\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), config={'run_name': 'format_inputs'})\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], template='\\nAnswer the following question using only the provided context.\\nthink step-by-step before generating a response.\\n<context>\\n{context}\\n</context>\\n\\n\\nQuestion:{input}'))])\n",
       "            | ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000022E88B1FCA0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000022E88B1EF80>, groq_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), config={'run_name': 'stuff_documents_chain'})\n",
       "  }), config={'run_name': 'retrieval_chain'})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=ret_chain.invoke({'input':\"Task Decomposition\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Task Decomposition',\n",
       " 'context': [Document(page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       "  Document(page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       "  Document(page_content='Chain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       "  Document(page_content='Memory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'})],\n",
       " 'answer': 'Task decomposition in the context of this text refers to the process of breaking down a complex task into smaller, manageable sub-tasks or steps. This can be done using a large language model (LLM) with simple prompting, task-specific instructions, or human inputs. An example of simple prompting for task decomposition is \"Steps for XYZ.\\\\n1.\" or \"What are the subgoals for achieving XYZ?\". Task-specific instructions and human inputs can also be used for more specialized tasks.\\n\\nChain of Thought (CoT) is a standard prompting technique that utilizes this concept of task decomposition. It instructs the model to \"think step by step\" to decompose hard tasks into smaller and simpler steps. This transformation of big tasks into multiple manageable tasks provides insights into the model\\'s thinking process.\\n\\nIn summary, task decomposition is a crucial aspect of complex problem-solving, and it involves breaking down a problem into smaller, manageable tasks or steps to facilitate efficient problem-solving.'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition in the context of this text refers to the process of breaking down a complex task into smaller, manageable sub-tasks or steps. This can be done using a large language model (LLM) with simple prompting, task-specific instructions, or human inputs. An example of simple prompting for task decomposition is \"Steps for XYZ.\\n1.\" or \"What are the subgoals for achieving XYZ?\". Task-specific instructions and human inputs can also be used for more specialized tasks.\n",
      "\n",
      "Chain of Thought (CoT) is a standard prompting technique that utilizes this concept of task decomposition. It instructs the model to \"think step by step\" to decompose hard tasks into smaller and simpler steps. This transformation of big tasks into multiple manageable tasks provides insights into the model's thinking process.\n",
      "\n",
      "In summary, task decomposition is a crucial aspect of complex problem-solving, and it involves breaking down a problem into smaller, manageable tasks or steps to facilitate efficient problem-solving.\n"
     ]
    }
   ],
   "source": [
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
